{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b05c990e",
   "metadata": {},
   "source": [
    "## Comparing Classifiers using Heatmaps - Episode 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dde3c7",
   "metadata": {},
   "source": [
    "\n",
    "The video tutorial with explanation can be found <a href=\"https://youtu.be/kVrRPIt8qrs\">here</a> \\\n",
    "The dataset used for this tutorial was obtained from this <a href=\"https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-020-01823-3#Sec21\">publication</a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb3839",
   "metadata": {},
   "source": [
    "### Support my work\n",
    "\n",
    "<a href=\"https://www.buymeacoffee.com/informatician\">buy me a coffee</a> \\\n",
    "<a href=\"https://www.paypal.com/paypalme/theinformatician?country.x=DE&locale.x=en_US\">paypal</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7202fe",
   "metadata": {},
   "source": [
    "### Classifiers\n",
    " - Adaboost\n",
    " - Extra Trees\n",
    " - K-Nearest Neighbor\n",
    " - Logistic Regression\n",
    " - Naive Bayes\n",
    " - Random Forests\n",
    " - Support Vector Machines\n",
    " - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36017fbb",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    " - Accuracy\n",
    " - F1-score\n",
    " - Precision\n",
    " - Recall\n",
    " - Cohen's kappa statistic\n",
    " - Matthews correlation coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2fdbbd",
   "metadata": {},
   "source": [
    "### Required Python libraries\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Matplotlib\n",
    "- Seaborn\n",
    "- Scikit\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3dde3",
   "metadata": {},
   "source": [
    "### Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0515dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#classification\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# performance metrics\n",
    "from sklearn.metrics import balanced_accuracy_score,f1_score,precision_score, recall_score\n",
    "from sklearn.metrics import cohen_kappa_score,matthews_corrcoef\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45976a3",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data directly from a github repository\n",
    "file_url='https://raw.githubusercontent.com/vappiah/Machine-Learning-Tutorials/main/datasets/malaria_clin_data.csv'\n",
    "\n",
    "dataframe=pd.read_csv(file_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e214ad93",
   "metadata": {},
   "source": [
    "\n",
    "### Data Exploration & Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c2d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are interested in the columns : 'Clinical_diagnosis' up to 'RBC_dist_width_Percent'\n",
    "#meaning we will subset the data from column 16 - the last column\n",
    "\n",
    "subset=dataframe.iloc[:,16:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f631f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop / remove all rows with missing values\n",
    "\n",
    "subset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22749d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the class\n",
    "\n",
    "subset['Clinical_Diagnosis'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e165b",
   "metadata": {},
   "source": [
    " \n",
    "### **Data preprocesing** \n",
    "This is done to put the data in an appropriate format before modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will now seperate the feature values from the class. we do this because scikit-learn requires that features and class are separated before parsing them to the classifiers.\n",
    "\n",
    "X=subset.iloc[:,1:]\n",
    "y=subset.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a01cb",
   "metadata": {},
   "source": [
    "\\\n",
    "**Encode labels**\n",
    "\n",
    "The labels for this data are categorical and we therefore have to convert them to numeric forms. This is referred to as encoding. Machine learning models usually require input data to be in numeric forms, hence we encoding the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's encode target labels (y) with values between 0 and n_classes-1.\n",
    "#encoding will be done using the LabelEncoder\n",
    "\n",
    "label_encoder=LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "y_encoded=label_encoder.transform(y)\n",
    "labels=label_encoder.classes_\n",
    "classes=np.unique(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44215d8f",
   "metadata": {},
   "source": [
    "\\\n",
    "**Data Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2466485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler=MinMaxScaler()\n",
    "X=min_max_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f68c4f6",
   "metadata": {},
   "source": [
    "\\\n",
    "**Data Splitting**\n",
    "\n",
    "We will now split the data into training and test subsets.\n",
    "The training data is initially parsed to the machine learning model. this is to enable the model to identify discriminatory patterns which can be used to make future predictions.\n",
    "The testing data is used to evaluate the model after the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fada994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and test sets\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y_encoded,test_size=0.2,random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c12999",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0ba2c",
   "metadata": {},
   "source": [
    "### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model evaluation\n",
    "\n",
    "def evaluate(model,xtest,ytest,name):\n",
    "    \n",
    "    ypred=model.predict(xtest)  \n",
    "\n",
    "    accuracy=np.round(balanced_accuracy_score(ytest,ypred),4)\n",
    "    \n",
    "    precision=np.round(precision_score(ytest,ypred,average = 'weighted'),4)\n",
    "\n",
    "    recall=np.round(recall_score(ytest,ypred,average = 'weighted'),4)\n",
    "    \n",
    "    f1score=np.round(f1_score(ytest,ypred,average = 'weighted'),4)\n",
    "    \n",
    "    cohenkappa_score=np.round(cohen_kappa_score(ytest,ypred),4)\n",
    " \n",
    "    matthews_corrcoef_=np.round(matthews_corrcoef(ytest,ypred),4)\n",
    "    \n",
    "    return accuracy,precision,recall,f1score,cohenkappa_score,matthews_corrcoef_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model fitting\n",
    "\n",
    "def fit_data(xtrain,ytrain,xtest,ytest):\n",
    "    \n",
    "    #Adaboost Classifier\n",
    "    ADB=AdaBoostClassifier()\n",
    "    ADB.fit(xtrain,ytrain)\n",
    "\n",
    "    #XGBoost Classifier\n",
    "    XGB=XGBClassifier(num_class=labels.shape,eval_metric='mlogloss',use_label_encoder =False)\n",
    "    XGB.fit(xtrain,ytrain)\n",
    "    \n",
    "    #Random Forest Classifier\n",
    "    RF=RandomForestClassifier(max_features=0.2)\n",
    "    RF.fit(xtrain,ytrain)\n",
    "\n",
    "    #Support Vector Machine Classifier\n",
    "    SVM=SVC()\n",
    "    SVM.fit(xtrain,ytrain)\n",
    "\n",
    "    #K-nearest Neighbor classifier\n",
    "    KNN=KNeighborsClassifier()\n",
    "    KNN.fit(xtrain,ytrain)\n",
    "\n",
    "    #Naive Bayes Classifier\n",
    "    NB=GaussianNB()\n",
    "    NB.fit(xtrain,ytrain)\n",
    "\n",
    "    #Extra Trees Classifier\n",
    "    ETC=ExtraTreesClassifier()\n",
    "    ETC.fit(xtrain,ytrain)\n",
    "\n",
    "    #Logistic Regression\n",
    "    \n",
    "    LOGREG =LogisticRegression(C = 50, multi_class = 'multinomial',solver='lbfgs', max_iter=3000)\n",
    "    LOGREG.fit(xtrain,ytrain)\n",
    "\n",
    "    #this list will be used to store the scores for all classifiers\n",
    "    performance_list=[]\n",
    "\n",
    "    #performance metrics to be used for evaluating the classifiers\n",
    "    performance_metrics=['Accuracy','Precision','Recall','F1score',\n",
    "                    'Cohen_kappa_score','Matthews coefficient']\n",
    "    indices=[]\n",
    "\t\n",
    "    #create a dictionary object to store the models\n",
    "    model_dict={'KNN':KNN,'Random Forest':RF,'LOGREG':LOGREG,'SVM':SVM,'Naive Bayes':NB,\n",
    "            'XGBoost':XGB,'Extra Trees':ETC,'Adaboost':ADB}\n",
    "\n",
    "    #evaluate the each model stored in the dictionary object\n",
    "    for name,model in model_dict.items():\n",
    "        performance=evaluate(model,xtest,ytest,name)\n",
    "        performance_list.append(performance)\n",
    "        indices.append(name)\n",
    "        \n",
    "    performance_frame=pd.DataFrame(performance_list,columns=performance_metrics,index=indices)\n",
    "    return performance_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634f2c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fit the models to the training data and evaluate the models\n",
    "#this is done by calling the functions created in the previous step\n",
    "#the result is assigned to a variable\n",
    "\n",
    "result=fit_data(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39276493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get classifiers with f1score greater or equal to 0.81\n",
    "result[result['F1score']>=0.81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14af569",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#use seaborn to generate heatmap using the result from the previous step\n",
    "plt.rcParams['figure.constrained_layout.use'] = True\n",
    "\n",
    "sns.heatmap(result,annot=True,cmap='Blues')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Classifiers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f3eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the scores to an output csv file\n",
    "result.to_csv('scores.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
